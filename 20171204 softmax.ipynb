{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = 'data/train_dataset.csv'\n",
    "test_dataset = 'data/test_dataset.csv' ## 저번시간에 사용한 csv 다시 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  0  1.1  2  3  4  5  6  7  8  9\n",
       "894  0  9    8  7  6  5  4  3  2  1  0\n",
       "895  1  0    1  2  3  4  5  6  7  8  9\n",
       "896  0  9    8  7  6  5  4  3  2  1  0\n",
       "897  1  0    1  2  3  4  5  6  7  8  9\n",
       "898  0  9    8  7  6  5  4  3  2  1  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reading = pd.read_csv('data/train_dataset.csv')\n",
    "reading.tail()\n",
    "### classes 가 0과 1로 이루어진 binary classification \n",
    "## softmax는 다항 multiply classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-139f5845060d>:1: TextLineDataset.__init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.TextLineDataset`.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.contrib.data.TextLineDataset(train_dataset).batch(20).repeat(10000)\n",
    "test_dataset = tf.contrib.data.TextLineDataset(test_dataset).batch(20).repeat(99999)\n",
    "\n",
    "train_itr = train_dataset.make_one_shot_iterator()\n",
    "test_itr = test_dataset.make_one_shot_iterator()\n",
    "\n",
    "train_batch = train_itr.get_next()\n",
    "test_batch = test_itr.get_next()\n",
    "\n",
    "train_batch = tf.decode_csv(train_batch, [[0]]*11)\n",
    "test_batch = tf.decode_csv(test_batch, [[0]]*11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = tf.reshape(train_batch[0],[-1,1])\n",
    "train_label = tf.cast(train_label, tf.float32)\n",
    "\n",
    "test_label = tf.reshape(test_batch[0],[-1,1])\n",
    "test_label = tf.cast(test_label, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label: Tensor(\"Cast:0\", shape=(?, 1), dtype=float32)\n",
      "train_feature:  Tensor(\"Cast_2:0\", shape=(?, 10), dtype=float32)\n",
      "**********************************************************************\n",
      "test_label: Tensor(\"Cast_1:0\", shape=(?, 1), dtype=float32)\n",
      "test_feature: Tensor(\"Cast_3:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_feature = tf.stack(train_batch[1:],axis = 1)\n",
    "train_feature = tf.cast(train_feature, tf.float32)\n",
    "\n",
    "test_feature = tf.stack(test_batch[1:], axis = 1)\n",
    "test_feature = tf.cast(test_feature, tf.float32)\n",
    "\n",
    "print('train_label:',train_label)\n",
    "print('train_feature: ',train_feature)\n",
    "print('*'*70)\n",
    "print('test_label:',test_label)\n",
    "print('test_feature:',test_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"outlayer/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"outlayer_2/BiasAdd:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def layers(x, activation = None, reuse = False):\n",
    "    \n",
    "    \n",
    "    layer1 = tf.layers.dense(x, units = 5, activation = activation, reuse = reuse, name = 'layer1')\n",
    "    layer2 = tf.layers.dense(layer1, units = 6, activation = activation, reuse =reuse, name = 'layer2')\n",
    "    layer3 = tf.layers.dense(layer2, units = 7, activation = activation, reuse =reuse, name = 'layer3')\n",
    "    layer4 = tf.layers.dense(layer3, units = 8, activation = activation, reuse =reuse, name = 'layer4')\n",
    "    out = tf.layers.dense(layer4, units = 1, reuse =reuse, name = 'outlayer')\n",
    "    \n",
    "    return out \n",
    "\n",
    "train_out = layers(train_feature)\n",
    "test_out = layers(test_feature, reuse = True)\n",
    "\n",
    "print(train_out)\n",
    "print(test_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'layer1/kernel:0' shape=(10, 5) dtype=float32_ref>\n",
      "<tf.Variable 'layer1/bias:0' shape=(5,) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/kernel:0' shape=(5, 6) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/bias:0' shape=(6,) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/kernel:0' shape=(6, 7) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/bias:0' shape=(7,) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/kernel:0' shape=(7, 8) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/bias:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'outlayer/kernel:0' shape=(8, 1) dtype=float32_ref>\n",
      "<tf.Variable 'outlayer/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.softmax_cross_entropy(train_label, train_out) ## 만약 softmax를 activation fuction으로 쓰고싶으면 cost 함수도 softmax를 써야함 \n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar ('loss', loss) ## loss 에 대해서 summary collecting\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred1 = tf.nn.sigmoid(test_out) \n",
    "pred2 = tf.nn.softmax(test_out)## prediction \n",
    "accuracy1 = tf.metrics.accuracy(test_label, tf.round(pred1))\n",
    "accuracy2 = tf.metrics.accuracy(test_label, tf.round(pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 ,loss:1.2892146110534668, Sigmoid accuracy:(0.0, 0.5), Softmax accuracy:(0.0, 0.5)\n",
      "step:100 ,loss:0.0011753430590033531, Sigmoid accuracy:(0.995, 0.99504948), Softmax accuracy:(0.5, 0.5)\n",
      "step:200 ,loss:0.0005422182730399072, Sigmoid accuracy:(0.9975, 0.99751246), Softmax accuracy:(0.5, 0.5)\n",
      "step:300 ,loss:0.0003448121715337038, Sigmoid accuracy:(0.99833333, 0.99833888), Softmax accuracy:(0.5, 0.5)\n",
      "step:400 ,loss:0.00025014509446918964, Sigmoid accuracy:(0.99874997, 0.99875313), Softmax accuracy:(0.5, 0.5)\n",
      "step:500 ,loss:0.00019505596719682217, Sigmoid accuracy:(0.99900001, 0.99900198), Softmax accuracy:(0.5, 0.5)\n",
      "step:600 ,loss:0.00015920696023385972, Sigmoid accuracy:(0.99916667, 0.99916804), Softmax accuracy:(0.5, 0.5)\n",
      "step:700 ,loss:0.0001341066526947543, Sigmoid accuracy:(0.9992857, 0.99928671), Softmax accuracy:(0.5, 0.5)\n",
      "step:800 ,loss:0.00011559783160919324, Sigmoid accuracy:(0.99937499, 0.99937576), Softmax accuracy:(0.5, 0.5)\n",
      "step:900 ,loss:0.00010141347593162209, Sigmoid accuracy:(0.99944443, 0.99944508), Softmax accuracy:(0.5, 0.5)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('data/',sess.graph)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        _,_loss, = sess.run([train_op,loss])\n",
    "        _summ = sess.run(merged)\n",
    "    \n",
    "        _pred1 = sess.run(pred1)\n",
    "        _pred2 = sess.run(pred2)\n",
    "        _accuracy1 = sess.run(accuracy1)\n",
    "        _accuracy2 = sess.run(accuracy2)\n",
    "        writer.add_summary(_summ,i)\n",
    "        \n",
    "        if i %100 ==0:\n",
    "            print('step:{} ,loss:{}, Sigmoid accuracy:{}, Softmax accuracy:{}'.format(i, _loss, _accuracy1,_accuracy2))\n",
    "            \n",
    "    saver.save(sess,'data/saved parameters') "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 결론: sigmoid로 test_out을 prediction해준결과 데이터에 따라 사용하는 activation 함수를 달리 사용해야한다는 결론 \n",
    "\n",
    "위 데이터 셋의 경우 일정하게 고르게 분포된 linear dataset이며 이에따라 0과 1의 classification을 가지는 logistic regression 이라고 할수 있음. 그래서 train 할때 activation function  없이 matmul(W,b)의 선형함수가 제일 잘 나오며 추후 prediction activation 함수에서 sigmoid 쓰는것이 제일 좋음\n",
    "\n",
    "반면, softmax는 무조건 제일 큰값을 값으로 내주기때문에 여기 선형회귀에서는 제일큰 숫자인 9를 계속 반환, 그래서 hidden layer에 activation 함수를 softmax로 쓰면 0.5의 accuracy로 수렴, 결과값도 제일 큰값인 9를 반환하여 0과1인 classification에서도 0.5확률만 나오는 이유가 그것임, prediction에 softmax를 써도 마찬가지임\n",
    "\n",
    "## 왜그런가 계속생각했음... 강사님... 데이터셋을 다른걸 쓰라는것이 이 숙제의 요지이셨나요? softmax만 쓰면 되는줄 알고 cost 함수 activation 함수..hidden layer, node개수 다 바꿔봤는데 안되어서 계속 이론을 생각해봤더니 위와 같은 결론이 나왔습니다.... \n",
    "\n",
    "## 다음에는 MNEST digit 이미지 데이터셋 만개는 너무 많고 일단 천개만 해보고 결론 드릴게요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
