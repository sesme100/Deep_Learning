{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is written\n",
      "test data is written\n"
     ]
    }
   ],
   "source": [
    "## csv 파일 만들기 \n",
    "\n",
    "samples = 1000\n",
    "test_samples = 100\n",
    "train_dataset = 'data/train_dataset.csv'\n",
    "test_dataset = 'data/test_dataset.csv'\n",
    "\n",
    "def write_dataset(samples, test_samples, train_dir, test_dir):\n",
    "    up = [i for i in range(10)]\n",
    "    down = [9-i for i in range(10)]\n",
    "\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(samples):\n",
    "        data.append(up)\n",
    "        data.append(down)\n",
    "        label.append([1])\n",
    "        label.append([0])\n",
    "        \n",
    "    with open(train_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(samples-test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('train data is written')\n",
    "\n",
    "    with open(test_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('test data is written')\n",
    "        \n",
    "write_dataset(1000, 100, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1.1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  0  1.1  2  3  4  5  6  7  8  9\n",
       "1  1  0    1  2  3  4  5  6  7  8  9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv('data/train_dataset.csv')\n",
    "data1[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-58af81bd371f>:3: TextLineDataset.__init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.TextLineDataset`.\n"
     ]
    }
   ],
   "source": [
    "## 데이터셋을 읽어오기 (batch 사이즈를 정해줄수 있음, 셔플도 가능함)\n",
    "train_dataset = tf.contrib.data.TextLineDataset(train_dataset).batch(32)#.shuffle(7777)\n",
    "train_dataset = train_dataset.repeat(100)\n",
    "## 셔플해도 상관없음\n",
    "\n",
    "test_dataset = tf.contrib.data.TextLineDataset(test_dataset).batch(32)\n",
    "test_dataset = test_dataset.repeat(99999999)\n",
    "## 경로 적어주고 경로에 있는 데이터를 받아서 가져다줌 ## batch 사이즈 32개 \n",
    "\n",
    "train_itr = train_dataset.make_one_shot_iterator() \n",
    "test_itr = test_dataset.make_one_shot_iterator() \n",
    "## iterator만들기: dataset 클레스도 내부에서 queue runners가 동작\n",
    "## iterator는 그 queue에 dequeue를 해주는 역할\n",
    "## batch가 실행될때마다 다음 batch 얻어옴 \n",
    "\n",
    "train_batch = train_itr.get_next()\n",
    "test_batch = test_itr.get_next()## 다음 batch 얻어옴 \n",
    "\n",
    "\n",
    "train_batch = tf.decode_csv(train_batch, [[0]]*11) \n",
    "test_batch = tf.decode_csv(test_batch, [[0]]*11)\n",
    "## tf.expand_dims(array, axis) - 차원추가 \n",
    "\n",
    "\n",
    "## decode를 안하면 dataset에서 reader로 TextLineDateset 메소드로 읽어왔기때문에 string type으로 출력됨 \n",
    "## decode_csv를 하면 숫자로 읽어올수 있음\n",
    "## Convert CSV records to tensors. Each column maps to one tensor\n",
    "## [[0]]*11 = record defaults\n",
    "## default를 integer 형, 값은 [0]으로 함 column들의 기본 형식을 정해준다 생각하면됨\n",
    "## 예 :record_defaults = [ [\"null\"],[1],[1900],[\"null\"],[\"null\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 613\n"
     ]
    }
   ],
   "source": [
    "[[0]]*11 ## 11 columns: integer 형식, default 값 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_label  = tf.reshape(train_batch[0],[-1,1])  \n",
    "train_label = tf.cast(train_label, tf.float32) \n",
    "## cast:정수형이면 tensorflow에서 실행되지 않아서 float로 바꿔줌 \n",
    "## reshape 함수에 사용된 -1은 나누어 떨어지는 숫자를 자동으로 적용하겠다는 뜻이다. \n",
    "## 예: 12개의 요소가 있을 때, [3,-1]이라는 것은 [3,4]와 같고, [2,2,-1]이라는 것은 [2,2,3]과 같다. \n",
    "## 행과 열, 페이지에서 중심이 되는 것을 지정한 다음에, 나머지를 계산하기 싫을 때 주로 사용된다.\n",
    "\n",
    "test_label  = tf.reshape(test_batch[0],[-1,1]) ## 899 rows *1 columns (배치사이즈에 맞게 조절됨 )\n",
    "test_label = tf.cast(test_label, tf.float32)\n",
    "\n",
    "\n",
    "# row_feature = tf.stack(decoded_batch[1:], axis = 0)\n",
    "train_feature = tf.stack(train_batch[1:], axis = 1)\n",
    "train_feature = tf.cast(train_feature, tf.float32)\n",
    "\n",
    "test_feature = tf.stack(test_batch[1:], axis = 1)\n",
    "test_feature = tf.cast(test_feature, tf.float32) ## float 형으로 바꿔줘야함 \n",
    "# cast : intiger이면 tensorflow에서 실행되지 않아서 float로 바꿔줌 \n",
    "#feature  = decoded_batch[1:] ## 1번째 이후 나머지 줄 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([[0],[1],[2],[3],[4],[5]]).reshape([-1,2]) \n",
    "## -1: 정해지지 않은 행을 뜻함, 정해진 열 두개에 대해 임의로 행 사이즈를 정해줌"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 참고\n",
    "x = tf.constant([1, 4])\n",
    "y = tf.constant([2, 5])\n",
    "z = tf.constant([3, 6])\n",
    "tf.stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)\n",
    "tf.stack([x, y, z], axis=1)  # [[1, 2, 3], [4, 5, 6]]\n",
    "##tf.stack([x, y, z]) = np.stack([x, y, z])\n",
    "data1[1:4]\n",
    "np.stack(data1[1:2],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  6]\n",
      " [ 2  7]\n",
      " [ 3  8]\n",
      " [ 4  9]\n",
      " [ 5 10]]\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]]\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "## 참고 \n",
    "a = np.array([[1,6],[2,7],[3,8],[4,9],[5,10]])\n",
    "print(np.stack(a, axis = 0))\n",
    "print(np.stack(a, axis = 1))## Transpose 한것처럼 쌓아줌  \n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### label = train_batch[0]\n",
    "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]) \n",
    "## 위에서는 reshape 해서 [batch_size, 1] 열컬럼으로 다시 정렬해줌 \n",
    "## Each column maps to one tensor when decode 디코딩할때 컬럼들을 텐서로 맵핑해줌 \n",
    "\n",
    "#### train_feature = tf.stack(decoded_batch[1:], axis = 0)## row 로 정렬 \n",
    "## 위에서는 axis = 1로  transpose한것처럼 쌓아줌\n",
    "array([0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0,9, 0, 9, 0, 9, 0, 9, 0, 9]), \n",
    "array([1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1,8, 1, 8, 1, 8, 1, 8, 1, 8]), \n",
    "array([2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2,7, 2, 7, 2, 7, 2, 7, 2, 7]), \n",
    "array([3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3,6, 3, 6, 3, 6, 3, 6, 3, 6]), \n",
    "array([4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4,5, 4, 5, 4, 5, 4, 5, 4, 5]), \n",
    "array([5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5,4, 5, 4, 5, 4, 5, 4, 5, 4]), \n",
    "array([6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6,3, 6, 3, 6, 3, 6, 3, 6, 3]), \n",
    "array([7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7,2, 7, 2, 7, 2, 7, 2, 7, 2]), \n",
    "array([8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8,1, 8, 1, 8, 1, 8, 1, 8, 1]), \n",
    "array([9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9,0, 9, 0, 9, 0, 9, 0, 9, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Cast_1:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Cast_2:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Cast_3:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(train_label)\n",
    "print(test_label)\n",
    "print(train_feature)\n",
    "print(test_feature)\n",
    "\n",
    "## Tensor(\"Cast_3:0\", shape=(?, 10), dtype=float32) \n",
    "## shape는 배치사이즈가 처음에 들어가긴 하지만  총데이터 899개를 씀, 그이상으로 몇번 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 shape (?, 5)\n",
      "layer2 shape (?, 6)\n",
      "layer3 shape (?, 7)\n",
      "layer4 shape (?, 8)\n",
      "out shape (?, 1)\n",
      "layer1 shape (?, 5)\n",
      "layer2 shape (?, 6)\n",
      "layer3 shape (?, 7)\n",
      "layer4 shape (?, 8)\n",
      "out shape (?, 1)\n",
      "<tf.Variable 'layer1/kernel:0' shape=(10, 5) dtype=float32_ref>\n",
      "<tf.Variable 'layer1/bias:0' shape=(5,) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/kernel:0' shape=(5, 6) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/bias:0' shape=(6,) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/kernel:0' shape=(6, 7) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/bias:0' shape=(7,) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/kernel:0' shape=(7, 8) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/bias:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'out/kernel:0' shape=(8, 1) dtype=float32_ref>\n",
      "<tf.Variable 'out/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "## model 정하고 train 시키고 다음에 test 에 대입하는 함수 \n",
    "\n",
    "def bin_model(x, activation =None, reuse = False):\n",
    "    ## 모델 정하기(layer 정하기)\n",
    "    layer1 = tf.layers.dense(x, units =5 , activation = activation, reuse = reuse, name = 'layer1') ## \n",
    "    layer2 = tf.layers.dense(layer1, units =6,activation = activation, reuse = reuse ,name = 'layer2')\n",
    "    layer3 = tf.layers.dense(layer2, units =7,activation = activation, reuse = reuse,name = 'layer3' )\n",
    "    layer4 = tf.layers.dense(layer3, units =8,activation = activation, reuse = reuse,name = 'layer4' )\n",
    "    out = tf.layers.dense(layer4, units =1, reuse = reuse, name = 'out' ) \n",
    "    ## output은 activation 함수 사용하면 안됨 (loss를 linear로 구해야하기때문)\n",
    "    ## unit =hidden layer의 크기 \n",
    "    \n",
    "## 활성함수 activation 함수는 시그모이드 함수를 잘 사용하지 않는다.\n",
    "## 학습이 되지 않음 왜냐! 시그모이드의 기울기는 기점을 중심으로 퍼질때 0이 되기때문 \n",
    "## loss 가 수렴하지 않아서 줄어들지 않음 \n",
    "    \n",
    "    print('layer1 shape {}'.format(layer1.shape))\n",
    "    print('layer2 shape {}'.format(layer2.shape))\n",
    "    print('layer3 shape {}'.format(layer3.shape))\n",
    "    print('layer4 shape {}'.format(layer4.shape))\n",
    "    print('out shape {}'.format(out.shape))\n",
    "    \n",
    "    return out \n",
    "    \n",
    "    \n",
    "\n",
    "train_out = bin_model(train_feature)\n",
    "test_out = bin_model(test_feature,reuse = True) \n",
    "## train에서 업데이트 된 가중치 W와 b를 test에 대하여 다시 사용 = reuse \n",
    "\n",
    "for v in tf.trainable_variables():## train에서 업데이트 된 가중치 W와 b\n",
    "    print(v)\n",
    "    \n",
    "    \n",
    "saver = tf.train.Saver() ## save all Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss 구하기 \n",
    "loss = tf.losses.sigmoid_cross_entropy(train_label,train_out)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "tf.summary.scalar ('loss', loss) ## loss 에 대해서 summary collecting\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tf.nn.sigmoid(test_out) ## prediction \n",
    "accuracy = tf.metrics.accuracy(test_label, tf.round(pred)) ## metrics = 검증 메트릭스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 ,loss:1.5148115158081055, accuracy(0.0, 0.5)\n",
      "step:100 ,loss:0.0028549295384436846, accuracy(0.99000001, 0.99009901)\n",
      "step:200 ,loss:0.0011999162379652262, accuracy(0.995, 0.99502486)\n",
      "step:300 ,loss:0.0007296655094251037, accuracy(0.99666667, 0.99667776)\n",
      "step:400 ,loss:0.0005145620089024305, accuracy(0.9975, 0.99750626)\n",
      "step:500 ,loss:0.0003931887913495302, accuracy(0.99800003, 0.99800402)\n",
      "step:600 ,loss:0.00031596189364790916, accuracy(0.99833333, 0.99833608)\n",
      "step:700 ,loss:0.0002628252259455621, accuracy(0.99857146, 0.99857348)\n",
      "step:800 ,loss:0.00022419585729949176, accuracy(0.99874997, 0.99875158)\n",
      "step:900 ,loss:0.00019494109437800944, accuracy(0.99888891, 0.9988901)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    ## 세션을 초기화 하는 순간 변수 W에 그 값이 지정되는데\n",
    "    ## 다음과 같이 변수들을 global_variables_initializer() 를 이용해서 초기화 한후, 초기화된 결과를 세션에 전달해 줘야 한다.\n",
    "\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('data/',sess.graph)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        _,_loss, = sess.run([train_op,loss])\n",
    "        _accuracy = sess.run(accuracy)\n",
    "        _summ = sess.run(merged)\n",
    "    \n",
    "        _pred = sess.run(pred)\n",
    "        writer.add_summary(_summ,i)\n",
    "        \n",
    "        if i %100 ==0:\n",
    "            print('step:{} ,loss:{}, accuracy{}'.format(i, _loss, _accuracy))\n",
    "    saver.save(sess,'data/saved parameters') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saver.save(sess,'data/saved parameters') \n",
    "## 세션에 돌아가는 그래프생성된 모든 정보들이 저장됨 \n",
    "## 확장자 이름 없음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: ## saver로 지금 있는 모든 정보 저장\n",
    "    saver.restore(sess, './data') ## 디렉토리만 넣어주면 됨 \n",
    "    _pred = sess.run(pred)\n",
    "    print(_pred)\n",
    "    \n",
    "## INFO:tensorflow:Restoring parameters from ./data 이라고 출력됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data 디렉토리로 tensorboard 로그연결\n",
    "#### window powershell 열기 data가 있는 윗 디렉토리 Deeplearing_Tensorflow 로 들어가서 하기의 command 입력 \n",
    "\n",
    "PS C:\\Users\\User\\DS School Course Materials\\Deeplearning_Tensorflow> tensorboard --logdir=data 입력 후 \n",
    "\n",
    "http://localhost:6006 크롬에서 접속 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 입력후 하기와 같이 나오는 것을 알수 있음 \n",
    "\n",
    "PS C:\\Users\\User\\DS School Course Materials\\Deeplearning_Tensorflow> tensorboard --logdir=data \n",
    "TensorBoard 0.4.0rc3 at http://DESKTOP-CV5522K:6006 (Press CTRL+C to quit)\n",
    "W1201 18:04:01.213831 Reloader tf_logging.py:86] Found more than one graph event per run, or there was a metagraph conta\n",
    "ining a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
    "W1201 18:04:01.213831 Reloader tf_logging.py:86] Found more than one metagraph event per run. Overwriting the metagraph\n",
    "with the newest event.\n",
    "W1201 18:04:01.308092 Reloader tf_logging.py:86] Found more than one graph event per run, or there was a metagraph conta\n",
    "ining a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
    "W1201 18:04:01.308092 Reloader tf_logging.py:86] Found more than one metagraph event per run. Overwriting the metagraph\n",
    "with the newest event.\n",
    "W1201 18:04:01.370642 Reloader tf_logging.py:86] Found more than one graph event per run, or there was a metagraph conta\n",
    "ining a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
    "W1201 18:04:01.370642 Reloader tf_logging.py:86] Found more than one metagraph event per run. Overwriting the metagraph\n",
    "with the newest event.\n",
    "W1201 18:04:01.417520 Reloader tf_logging.py:86] Found more than one graph event per run, or there was a metagraph conta\n",
    "ining a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
    "W1201 18:04:01.433106 Reloader tf_logging.py:86] Found more than one metagraph event per run. Overwriting the metagraph\n",
    "with the newest event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 궁금한점: placeholder, Variable 함수 사용안함 \n",
    "# 그러나 tf.layers.dense 사용후 프린트 하면 나옴 ,, 왜그러지?  오토 레이어라고 함 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## hidden layer + out 개수\n",
    "처음 5개 \n",
    "## hidden layer unit 개수 \n",
    "[5,6,7,8,1]\n",
    "## activation fuction 종류\n",
    "사용하지 않았을때 좋음:step:100 ,loss:0.0022177849896252155, accuracy(1.0, 1.0)\n",
    "tf.nn.sigmoid :step:900 ,loss:0.6931887865066528, accuracy(0.44944444, 0.44950056)\n",
    "tf.nn.tanh\n",
    "tf.nn.softmax\n",
    "tf.nn.relu\n",
    "tf.nn.Maxout\n",
    "tf.nn.ELU\n",
    "tf.nn.Leaky_ReLU\n",
    "tf.nn.dropout\n",
    "tf.nn.max_pool\n",
    "\n",
    "## pred activation 함수 \n",
    "tf.nn.sigmoid 사용:\n",
    "tf.nn.softmax :step:100 ,loss:0.004737465176731348, accuracy(0.5, 0.5)\n",
    "\n",
    "## cost fuction 종류\n",
    "tf.losses.sigmoid_cross_entropy\n",
    "tf.losses.mean_sqaured_error\n",
    "tf.losses.log_loss\n",
    "\n",
    "## step size \n",
    "주로 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
